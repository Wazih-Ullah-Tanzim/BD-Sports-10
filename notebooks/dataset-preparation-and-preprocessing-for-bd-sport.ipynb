{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":13228710,"sourceType":"datasetVersion","datasetId":8385204}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"\n\n# Title: Dataset Preparation and Preprocessing for BD Sports-10\n\n\n## Dataset Access:\n\n\n###  We have used the resized version (224×224 pixels) of the dataset for experimentation. However, you can also access the original high-resolution version (1920×1080 pixels).\n\n\n### 1) Resized Version (224×224): Mendeley Data: https://data.mendeley.com/datasets/rnh3x48nfb/1\n\n### 2) Original Version (1920×1080): Science Data Bank: https://doi.org/10.57760/sciencedb.24216\n\n  \n\n## Citation:\n\n\n### When using this dataset, please do not forget to cite the following:\n\n\n#### 1) Tanzim, Wazih Ullah; Minhaz Hossain, Syed Md.; Supta, Niloy Barua; Shifa, Shifatun Nur (2025).DOI: 10.17632/rnh3x48nfb.1\n\n\n\n#### 2) Wazih Ullah Tanzim, Syed Md. Minhaz Hossain, Niloy Barua Supta, et al. (2025).DOI: 10.57760/sciencedb.24216\n\n\n\n\n\n\n### Step-by-Step Description\n\n1. **Install dependencies**\n\n   * Install required Python libraries:\n\n     ```bash\n     !pip install --upgrade tensorflow\n     !pip install -q git+https://github.com/tensorflow/docs\n     !pip install torch torchvision pillow\n     ```\n   * These packages enable deep learning (`tensorflow`, `torch`), dataset visualization (`tensorflow-docs`), and image/video handling (`pillow`, `opencv-python`).\n\n2. **Import necessary libraries**\n\n   * `cv2` for video frame extraction and resizing.\n   * `tensorflow` and `keras` for data preprocessing and model-ready tensors.\n   * `numpy` for numerical operations.\n   * `pandas` for handling metadata.\n   * `tqdm` for progress bars.\n   * `sklearn.model_selection.train_test_split` for splitting dataset.\n\n3. **Define configuration**\n\n   * Set global hyperparameters:\n\n     * Epochs: `100`\n     * Batch size: `32`\n     * Classes: `10` categories of Bangladeshi indigenous sports (Hari Vanga, Joldanga, Kabaddi, Kanamachi, Kho Kho, Kolagach, Lathim, Lathi Khela, Morog Lorai, Nouka Baich).\n   * `num_classes` automatically calculated from the class list.\n\n4. **Frame formatting**\n\n   * Each frame is resized to **224×224 pixels with padding** using TensorFlow.\n   * This ensures uniform input dimensions while preserving aspect ratio.\n\n5. **Frame extraction from videos**\n\n   * Videos are read using `cv2.VideoCapture`.\n   * From each video, a fixed number of **10 frames** are extracted.\n   * Frames are sampled with a `frame_step=15` to avoid redundancy.\n   * If the video is shorter than required, zero-padded frames are added.\n   * Frames are converted from **BGR → RGB** format for compatibility with deep learning models.\n\n6. **Train–test split**\n\n   * Videos from each class folder are collected (`.mp4` format).\n   * Data is shuffled for randomness.\n   * An **80–20 split** is applied:\n\n     * **80% training set**\n     * **20% test set**\n   * Targets (class labels) are stored as numeric indices.\n\n7. **Feature extraction**\n\n   * For each video in training and test sets:\n\n     * Frames are extracted.\n     * Each video is represented as a **NumPy array of shape**.\n   * This forms a dataset ready for model input.\n\n8. **Validation split**\n\n   * Training set is further split into **training (80%)** and **validation (20%)** using `train_test_split`.\n   * Ensures independent validation for model performance.\n\n9. **Convert to TensorFlow Datasets**\n\n   * Training, validation, and test arrays are converted into `tf.data.Dataset` objects.\n   * Each dataset is:\n\n     * **Shuffled** for randomness.\n     * **Batched** according to `CFG.batch_size`.\n     * **Cached & Prefetched** to optimize GPU training speed.\n\n10. **Final dataset structure**\n\n    * **Training dataset**: videos → frames → tensors.\n    * **Validation dataset**: subset of training set for tuning.\n    * **Test dataset**: unseen videos for evaluation.\n    * All inputs standardized to **(224×224×3)**, with **10 frames per video**.\n\n---\n","metadata":{}},{"cell_type":"code","source":"pip install --upgrade tensorflow","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import tensorflow as tf\nimport tensorflow_hub as hub\nimport keras\n\nprint(\"TensorFlow version:\", tf.__version__)\nprint(\"TensorFlow Hub version:\", hub.__version__)\nprint(\"Keras version:\", keras.__version__)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"pip install -q git+https://github.com/tensorflow/docs","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import glob\nimport pandas as pd\nimport cv2\nimport gc\nimport numpy as np\nimport random\nimport imageio\nimport tensorflow as tf\nfrom sklearn.model_selection import train_test_split\nfrom tqdm.notebook import tqdm\nfrom tensorflow_docs.vis import embed\nimport matplotlib.pyplot as plt\nimport imageio\nfrom IPython.display import display, Image","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"pip install torch torchvision","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"pip install pillow","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class CFG:\n    epochs = 100\n    batch_size = 32\n    classes = [\"Hari_Vanga\",\"Joldanga\", \"Kabaddi\", \"Kanamachi\",\"Kho_Kho\", \"Kolagach\",\"Lathim\",\"Lathi_Khela\",\"Morog_Lorai\", \"Nouka_Baich\"]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"num_classes = len(CFG.classes)\nprint(num_classes)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Convert Image Dtype and Resize with Padding","metadata":{}},{"cell_type":"code","source":"def format_frames(frame, output_size):\n  \"\"\"\n    Pad and resize an image from a video.\n\n    Args:\n      frame: Image that needs to resized and padded. \n      output_size: Pixel size of the output frame image.\n\n    Return:\n      Formatted frame with padding of specified output size.\n  \"\"\"\n  frame = tf.image.convert_image_dtype(frame, tf.float32)\n  frame = tf.image.resize_with_pad(frame, *output_size)\n  return frame","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Frame Extraction from Video","metadata":{}},{"cell_type":"code","source":"def frames_from_video_file(video_path, n_frames, output_size = (224,224), frame_step = 15):\n  \"\"\"\n    Creates frames from each video file present for each category.\n\n    Args:\n      video_path: File path to the video.\n      n_frames: Number of frames to be created per video file.\n      output_size: Pixel size of the output frame image.\n\n    Return:\n      An NumPy array of frames in the shape of (n_frames, height, width, channels).\n  \"\"\"\n  # Read each video frame by frame\n  result = []\n  src = cv2.VideoCapture(str(video_path))  \n\n  video_length = src.get(cv2.CAP_PROP_FRAME_COUNT)\n\n  need_length = 1 + (n_frames - 1) * frame_step\n\n  if need_length > video_length:\n    start = 0\n  else:\n    max_start = video_length - need_length\n    start = random.randint(0, max_start + 1)\n\n  src.set(cv2.CAP_PROP_POS_FRAMES, start)\n  # ret is a boolean indicating whether read was successful, frame is the image itself\n  ret, frame = src.read()\n  result.append(format_frames(frame, output_size))\n\n  for _ in range(n_frames - 1):\n    for _ in range(frame_step):\n      ret, frame = src.read()\n    if ret:\n      frame = format_frames(frame, output_size)\n      result.append(frame)\n    else:\n      result.append(np.zeros_like(result[0]))\n  src.release()\n  result = np.array(result)[..., [2, 1, 0]]\n\n  return result","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Train Test Split","metadata":{}},{"cell_type":"code","source":"import os\nimport random\n\ntrain_file_paths = []\ntest_file_paths = []\n\ntrain_targets = []\ntest_targets = []\n\n# For reproducibility\nrandom.seed(42)\n\n# Assuming CFG.classes contains the class names in order\nfor i, cls in enumerate(CFG.classes):\n    class_dir = f\"/kaggle/input/bd-sports-10-dataset-224x224-pixels-resized/BD_Sports_10/Dataset/{cls}\"\n    \n    if os.path.exists(class_dir):\n        # Collect all mp4 files from this class\n        files = [os.path.join(class_dir, f) for f in os.listdir(class_dir) if f.endswith('.mp4')]\n        \n        # Shuffle before splitting\n        random.shuffle(files)\n        \n        # 80% train, 20% test split\n        split_idx = int(0.8 * len(files))\n        train_files = files[:split_idx]\n        test_files = files[split_idx:]\n        \n        # Add to lists\n        train_file_paths.extend(train_files)\n        test_file_paths.extend(test_files)\n        \n        train_targets.extend([i] * len(train_files))\n        test_targets.extend([i] * len(test_files))\n\n# Print sample file paths and targets to verify\nprint(\"\\033[1mTraining file paths:\\033[0m\")\nprint(train_file_paths[:5])\n\nprint(\"\\n\\033[1mTest file paths:\\033[0m\")\nprint(test_file_paths[:5])\n\nprint(\"\\n\\033[1mTrain Targets (first few):\\033[0m\")\nprint(train_targets)\n\nprint(\"\\n\\033[1mTest Targets (first few):\\033[0m\")\nprint(test_targets)\n\n# Print total counts\nprint(\"\\n\\033[1mTotal Training files:\\033[0m\", len(train_file_paths))\nprint(\"\\033[1mTotal Test files:\\033[0m\", len(test_file_paths))\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Extract Train Feature","metadata":{}},{"cell_type":"code","source":"train_features = []\nfor train_file_path in tqdm(train_file_paths):\n    train_features.append(frames_from_video_file(train_file_path, n_frames = 10))\ntrain_features = np.array(train_features)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Extract Test Feature","metadata":{}},{"cell_type":"code","source":"test_features = []\nfor test_file_path in tqdm(test_file_paths):\n    test_features.append(frames_from_video_file(test_file_path, n_frames = 10))\ntest_features = np.array(test_features)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Creating training feature, training targets, validation feature, validation targets","metadata":{}},{"cell_type":"code","source":"training_features, validation_features, training_targets,validation_targets = train_test_split(train_features, train_targets, test_size=0.2, random_state=42)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\n    f\"Training features shape:    {training_features.shape}\\n\"\n    f\"Validation features shape:  {validation_features.shape}\\n\"\n    f\"Training targets length:    {len(training_targets)}\\n\"\n    f\"Validation targets length:  {len(validation_targets)}\"\n)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\n    f\"Test features shape: {test_features.shape}\\n\"\n    f\"Test targets length: {len(test_targets)}\"\n)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Prepare Train and Validation Data","metadata":{}},{"cell_type":"code","source":"train_ds = tf.data.Dataset.from_tensor_slices((training_features, training_targets)).shuffle(CFG.batch_size * 4).batch(CFG.batch_size).cache().prefetch(tf.data.AUTOTUNE)\n\nvalid_ds = tf.data.Dataset.from_tensor_slices((validation_features,validation_targets)).batch(CFG.batch_size).cache().prefetch(tf.data.AUTOTUNE)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Prepare Test Data","metadata":{}},{"cell_type":"code","source":"# Prepare test dataset\ntest_ds = tf.data.Dataset.from_tensor_slices((test_features, test_targets)).batch(CFG.batch_size).cache().prefetch(tf.data.AUTOTUNE)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Now, The Dataset is Totally Prepared to fit into the Deep Learning Model","metadata":{}}]}